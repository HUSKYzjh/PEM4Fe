{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建文件夹\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def create_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        print(\"文件夹创建成功\")\n",
    "    else:\n",
    "        print(\"文件夹已存在\")\n",
    "pwd=\"/home/jhzhai/Nucleation/peek-find/cool700/nc2600\"\n",
    "# pwd=\"/home/jhzhai/Nucleation/peek-find/cool800/nc1600\"\n",
    "# pwd=\"/home/jhzhai/Nucleation/peek-find/cool900/nc1200\"\n",
    "# pwd=\"/home/jhzhai/Nucleation/peek-find/cool1000/nc800\"\n",
    "# pwd=\"/home/jhzhai/Nucleation/peek-find/cool1100/nc600\"\n",
    "create_folder(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder structure\n",
    "folder_structure = [\n",
    "    \"PEM4Fe/src\",\n",
    "    \"PEM4Fe/src/__init__.py\",\n",
    "    \"PEM4Fe/src/config.py\",\n",
    "    \"PEM4Fe/src/data_processing.py\",\n",
    "    \"PEM4Fe/src/fitting.py\",\n",
    "    \"PEM4Fe/src/peak_detection.py\",\n",
    "    \"PEM4Fe/src/visualization.py\",\n",
    "    \"PEM4Fe/src/output.py\",\n",
    "    \"PEM4Fe/src/main.py\",\n",
    "    \"PEM4Fe/tests\",\n",
    "    \"PEM4Fe/tests/__init__.py\",\n",
    "    \"PEM4Fe/tests/test_config.py\",\n",
    "    \"PEM4Fe/tests/test_data_processing.py\",\n",
    "    \"PEM4Fe/tests/test_fitting.py\",\n",
    "    \"PEM4Fe/tests/test_peak_detection.py\",\n",
    "    \"PEM4Fe/tests/test_visualization.py\",\n",
    "    \"PEM4Fe/tests/test_output.py\",\n",
    "    \"PEM4Fe/config.json\",\n",
    "    \"PEM4Fe/requirements.txt\",\n",
    "    \"PEM4Fe/README.md\"\n",
    "]\n",
    "\n",
    "# Create the folders and files\n",
    "for path in folder_structure:\n",
    "    if path.endswith('.py') or path.endswith('.json') or path.endswith('.txt') or path.endswith('.md'):\n",
    "        # Create file\n",
    "        with open(path, 'w') as f:\n",
    "            pass\n",
    "    else:\n",
    "        # Create directory\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(\"文件夹结构已创建\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是进一步优化代码的建议，整理为列表形式：\n",
    "\n",
    "---\n",
    "\n",
    "### **路径依赖性改进**\n",
    "1. **参数化路径**：\n",
    "   - 使用配置文件（如 JSON、YAML）或命令行参数传递路径，避免硬编码路径。\n",
    "   - 示例：通过 `argparse` 模块让用户在运行脚本时指定路径。\n",
    "2. **动态路径检测**：\n",
    "   - 检测当前运行环境的路径，并根据环境自动调整路径设置。\n",
    "\n",
    "---\n",
    "\n",
    "### **默认参数配置**\n",
    "1. **参数动态调整**：\n",
    "   - 引入基于数据特征的动态参数调整（如根据数据分布自动设置 `width_threshold` 和 `num_density`）。\n",
    "2. **参数文件化**：\n",
    "   - 将所有默认参数放入配置文件，方便统一管理和修改。\n",
    "3. **用户可调参数**：\n",
    "   - 提供命令行或交互界面，让用户动态修改关键参数。\n",
    "\n",
    "---\n",
    "\n",
    "### **性能优化**\n",
    "1. **启用并行处理**：\n",
    "   - 使用 `joblib` 的 `Parallel` 和 `delayed` 并行化对每一列数据的处理。\n",
    "   - 示例：将 `process_column` 调用包裹在 `Parallel` 中处理。\n",
    "2. **数据分块处理**：\n",
    "   - 对大数据集按时间或行分块处理，减少内存占用。\n",
    "3. **优化绘图性能**：\n",
    "   - 仅在需要时生成图像，避免不必要的重复计算和绘制。\n",
    "\n",
    "---\n",
    "\n",
    "### **异常处理改进**\n",
    "1. **文件异常**：\n",
    "   - 捕获文件读取异常（如文件不存在、格式不正确），并输出清晰的错误提示。\n",
    "2. **参数异常**：\n",
    "   - 检查用户输入参数是否合理（如 `width_threshold` 和 `num_density` 是否超出范围）。\n",
    "3. **拟合异常**：\n",
    "   - 对指数拟合失败的列标记为警告，而不是终止整个脚本。\n",
    "4. **日志记录**：\n",
    "   - 使用 `logging` 模块记录异常信息和运行状态，方便调试。\n",
    "\n",
    "---\n",
    "\n",
    "### **模块化设计**\n",
    "1. **功能拆分**：\n",
    "   - 将指数拟合、峰值检测和绘图功能提取为独立模块或类。\n",
    "   - 示例：创建 `FittingModule` 和 `PeakDetectionModule` 两个独立模块。\n",
    "2. **增加复用性**：\n",
    "   - 将通用功能（如数据清洗、异常处理）设计为工具函数，支持其他项目复用。\n",
    "3. **测试覆盖**：\n",
    "   - 针对模块化后的代码，编写单元测试以覆盖所有主要功能。\n",
    "\n",
    "---\n",
    "\n",
    "### **用户体验改进**\n",
    "1. **增强可视化**：\n",
    "   - 添加更多图表交互性（如 `plotly` 支持放大和查看详细数据点）。\n",
    "   - 在分析图中添加标注和颜色区分，提高可读性。\n",
    "2. **动态运行反馈**：\n",
    "   - 在处理大数据时，显示每个列的处理进度（如使用 `tqdm` 模块）。\n",
    "3. **错误修复提示**：\n",
    "   - 如果出现错误，提供修复建议，而不仅仅是报错信息。\n",
    "\n",
    "---\n",
    "\n",
    "### **数据格式扩展**\n",
    "1. **支持更多数据格式**：\n",
    "   - 增加对 CSV、Excel 和 HDF5 等数据格式的支持。\n",
    "2. **灵活数据输入**：\n",
    "   - 让用户指定数据格式和列名，而不是强制依赖固定命名规则。\n",
    "\n",
    "---\n",
    "\n",
    "### **代码维护与扩展**\n",
    "1. **添加注释和文档**：\n",
    "   - 为每个函数添加详细注释，包括参数解释和返回值说明。\n",
    "   - 提供完整的用户文档，指导如何使用脚本及调整参数。\n",
    "2. **代码版本控制**：\n",
    "   - 使用 `git` 或其他版本控制工具，管理代码的修改历史。\n",
    "3. **持续集成**：\n",
    "   - 引入自动化测试（如 `GitHub Actions`），确保代码修改不会引入新问题。\n",
    "\n",
    "---\n",
    "\n",
    "### **总结**\n",
    "通过以上优化措施，代码的可移植性、性能、可读性和用户体验将大大提升，同时为未来的功能扩展奠定基础。如果有任何具体问题，欢迎进一步讨论！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import minimize\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------- 配置模块 -------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- 配置模块 -------------------------- #\n",
    "def load_config(config_file):\n",
    "    \"\"\"\n",
    "    加载配置文件（支持 JSON 和 YAML）。\n",
    "    \n",
    "    参数:\n",
    "    - config_file (str): 配置文件的路径。\n",
    "    \n",
    "    返回:\n",
    "    - dict: 包含配置参数的字典。\n",
    "    \"\"\"\n",
    "    _, ext = os.path.splitext(config_file)\n",
    "    try:\n",
    "        with open(config_file, 'r') as file:\n",
    "            if ext == '.json':\n",
    "                return json.load(file)\n",
    "            elif ext in ('.yaml', '.yml'):\n",
    "                return yaml.safe_load(file)\n",
    "            else:\n",
    "                raise ValueError(\"不支持的配置文件格式，请使用 JSON 或 YAML。\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"配置文件 {config_file} 不存在，请检查路径。\")\n",
    "    except (json.JSONDecodeError, yaml.YAMLError):\n",
    "        raise ValueError(f\"配置文件 {config_file} 格式错误，无法解析。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "# config = load_config(\"/home/jhzhai/Nucleation/peek-find/config.yaml\")  # 或者 \"config.json\"\n",
    "config = load_config(\"C:\\\\Users\\\\husky\\\\Desktop\\\\peek-find\\\\PEM4Fe\\\\data\\\\config.yaml\")  # 或者 \"config.json\"\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments(args=None):\n",
    "    \"\"\"\n",
    "    解析命令行参数，支持 Jupyter Notebook。\n",
    "    \n",
    "    参数:\n",
    "    - args (list): 自定义参数列表（默认为 None，使用 sys.argv）。\n",
    "    \n",
    "    返回:\n",
    "    - argparse.Namespace: 包含解析后的参数对象。\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"解析脚本运行参数\")\n",
    "    \n",
    "    # 添加参数\n",
    "    parser.add_argument(\"--config\", type=str, default=\"config.json\", help=\"配置文件的路径（默认为 config.json）\")\n",
    "    parser.add_argument(\"--input_dir\", type=str, help=\"输入数据文件的目录，覆盖配置文件中的 input_dir 参数\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, help=\"输出结果保存的目录，覆盖配置文件中的 output_dir 参数\")\n",
    "    parser.add_argument(\"--Nsc\", type=float, help=\"Nsc 参数，覆盖配置文件中的 Nsc 值\")\n",
    "    parser.add_argument(\"--N0\", type=float, help=\"N0 参数，覆盖配置文件中的 N0 值\")\n",
    "    parser.add_argument(\"--pressure\", type=float, help=\"pressure 参数，覆盖配置文件中的 pressure 值\")\n",
    "    \n",
    "    # 如果在 Jupyter Notebook 中运行，传入自定义参数\n",
    "    if args is None:\n",
    "        args = sys.argv[1:]  # 默认从命令行读取参数\n",
    "\n",
    "    # 覆写配置文件并保存yaml文件\n",
    "    args = parser.parse_args(args)\n",
    "    if args.input_dir:\n",
    "        config[\"input_dir\"] = args.input_dir\n",
    "    if args.output_dir:\n",
    "        config[\"output_dir\"] = args.output_dir\n",
    "    if args.Nsc:\n",
    "        config[\"Nsc\"] = args.Nsc\n",
    "    if args.N0:\n",
    "        config[\"N0\"] = args.N0\n",
    "    if args.pressure:\n",
    "        config[\"pressure\"] = args.pressure\n",
    "    # with open(r\"C:\\Users\\husky\\Desktop\\peek-find\\PEM4Fe\\data\\config.yaml\", \"w\") as f:\n",
    "    #     yaml.dump(config, f)\n",
    "\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "if __name__ == \"__main__\":\n",
    "    # 自定义参数列表（模拟命令行）\n",
    "    # notebook_args = [\n",
    "    #     \"--config\", \"config.json\",\n",
    "    #     \"--input_dir\", \"/home/jhzhai/Nucleation/peek-find/cool700/nc2600\",\n",
    "    #     \"--output_dir\", \"/home/jhzhai/Nucleation/peek-find/cool700/nc2600\",\n",
    "    #     \"--Nsc\", \"100.0\",\n",
    "    #     \"--N0\", \"50.0\"\n",
    "    # ]\n",
    "    notebook_args = [\n",
    "        \"--config\", \"config.json\",\n",
    "        \"--pressure\",\"360\",\n",
    "        \"--Nsc\", \"3900.0\",\n",
    "        \"--N0\", \"2600.0\",\n",
    "        \n",
    "\n",
    "    ]\n",
    "    args = parse_arguments(notebook_args)\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir=args.input_dir\n",
    "output_dir=args.output_dir\n",
    "Press=args.pressure\n",
    "\n",
    "input_dir=config['input_dir']\n",
    "output_dir=config['output_dir']\n",
    "Press=config['pressure']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(output_dir=\"./logs\", log_file=\"app.log\", level=logging.INFO):\n",
    "    \"\"\"\n",
    "    设置日志记录。\n",
    "    \n",
    "    参数:\n",
    "    - output_dir (str): 日志文件保存的目录，默认是 \"./logs\"。\n",
    "    - log_file (str): 日志文件名，默认保存为 \"app.log\"。\n",
    "    - level (int): 日志级别，默认是 INFO。\n",
    "    \n",
    "    返回:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # 确保日志目录存在\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # 完整日志文件路径\n",
    "    log_file_path = os.path.join(output_dir, log_file)\n",
    "\n",
    "    # 清理旧的日志配置\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "\n",
    "    # 配置日志格式\n",
    "    log_format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "\n",
    "    # 配置日志记录\n",
    "    logging.basicConfig(\n",
    "        level=level,  # 设置日志级别\n",
    "        format=log_format,  # 设置日志格式\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file_path),  # 将日志写入文件\n",
    "            logging.StreamHandler()             # 同时输出到控制台\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 测试日志\n",
    "    logging.info(f\"日志文件保存路径: {log_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置日志保存目录和文件名\n",
    "    log_file = \"application.log\"\n",
    "    \n",
    "    # 调用日志设置函数\n",
    "    setup_logging(output_dir=output_dir, log_file=log_file, level=logging.DEBUG)\n",
    "\n",
    "    # 测试日志输出\n",
    "    logging.debug(\"DEBUG log test\")\n",
    "    logging.info(\"INFO log test\")\n",
    "    logging.warning(\"WARNING log test\")\n",
    "    logging.error(\"ERROR log test\")\n",
    "    logging.critical(\"CRITICAL log test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------- 数据处理模块 -------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data_from_files(directory):\n",
    "    \"\"\"\n",
    "    从指定目录中收集所有符合命名规则的文本文件，并合并为一个 DataFrame。\n",
    "    \n",
    "    参数:\n",
    "    - directory (str): 数据文件所在目录路径。\n",
    "    \n",
    "    返回:\n",
    "    - pd.DataFrame: 合并后的数据框，包含时间列 't' 和多个 'N_' 列。\n",
    "    \"\"\"\n",
    "    logging.info(f\"开始从目录 {directory} 中收集数据文件...\")\n",
    "    combined_df = pd.DataFrame()\n",
    "    file_count = 0\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        logging.error(f\"指定的目录 {directory} 不存在。\")\n",
    "        return combined_df\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith('pem-Nc-') and filename.endswith('.txt'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            try:\n",
    "                suffix = filename.split('-')[-1].split('.')[0]\n",
    "                # 使用原始字符串避免转义警告\n",
    "                df = pd.read_csv(filepath, sep=r'\\s+', names=['t', f'N_{suffix}'])\n",
    "                if combined_df.empty:\n",
    "                    combined_df = df\n",
    "                else:\n",
    "                    combined_df = pd.merge(combined_df, df, on='t', how='outer')\n",
    "                file_count += 1\n",
    "                logging.info(f\"成功读取文件: {filepath}\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"读取文件 {filepath} 失败: {e}\")\n",
    "\n",
    "    if file_count == 0:\n",
    "        logging.warning(f\"目录 {directory} 中没有找到符合命名规则的文件。\")\n",
    "    else:\n",
    "        logging.info(f\"共处理了 {file_count} 个文件。\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "def clean_data(t, N):\n",
    "    \"\"\"\n",
    "    清洗数据，去除无效值（NaN 和无穷值）。\n",
    "    \n",
    "    参数:\n",
    "    - t (pd.Series): 时间序列。\n",
    "    - N (pd.Series): 数据序列。\n",
    "    \n",
    "    返回:\n",
    "    - pd.Series: 清洗后的时间序列。\n",
    "    - pd.Series: 清洗后的数据序列。\n",
    "    \"\"\"\n",
    "    logging.info(\"开始清洗数据...\")\n",
    "    initial_length = len(N)\n",
    "\n",
    "    # 去除无效值\n",
    "    clean_mask = np.isfinite(N)\n",
    "    t_clean = t[clean_mask]\n",
    "    N_clean = N[clean_mask]\n",
    "\n",
    "    # 重置索引\n",
    "    t_clean = t_clean.reset_index(drop=True)\n",
    "    N_clean = N_clean.reset_index(drop=True)\n",
    "\n",
    "    cleaned_length = len(N_clean)\n",
    "    logging.info(f\"数据清洗完成：原始数据点 {initial_length} 个，清洗后数据点 {cleaned_length} 个。\")\n",
    "    \n",
    "    return t_clean, N_clean\n",
    "\n",
    "def load_clean_save_data(directory_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    加载、清洗并保存数据。\n",
    "    \n",
    "    参数:\n",
    "    - directory_path (str): 数据文件所在目录路径。\n",
    "    - output_csv_path (str): 保存合并后数据的 CSV 文件名。\n",
    "    \n",
    "    返回:\n",
    "    - pd.DataFrame: 清洗后的数据框。\n",
    "    - float: 从目录名中提取的 nc 值。\n",
    "    \"\"\"\n",
    "    # 提取目录名中的 nc 值\n",
    "    base_dir = os.path.basename(directory_path)\n",
    "    try:\n",
    "        nc_str = [part for part in base_dir.split('/') if 'nc' in part][0]\n",
    "        nc = float(nc_str.replace('nc', ''))\n",
    "    except (IndexError, ValueError):\n",
    "        logging.error(\"无法从目录名中提取 'nc' 值，请确保目录名包含 'nc' 后跟数值，例如 'nc1000'\")\n",
    "        nc = None\n",
    "    \n",
    "    collected_data = collect_data_from_files(directory_path)\n",
    "\n",
    "    output_csv_path=output_csv_path+'\\pem_N.csv'\n",
    "    save_to_csv(collected_data, os.path.join(directory_path, output_csv_path))\n",
    "    logging.info(f\"nc={nc}已处理完毕\")\n",
    "    df = pd.read_csv(os.path.join(directory_path, output_csv_path))\n",
    "    return df, nc\n",
    "\n",
    "def save_to_csv(dataframe, output_file):\n",
    "    \"\"\"\n",
    "    将 DataFrame 保存为 CSV 文件。\n",
    "    \n",
    "    参数:\n",
    "    - dataframe (pd.DataFrame): 要保存的数据框。\n",
    "    - output_file (str): 输出 CSV 文件路径。\n",
    "    \"\"\"\n",
    "    dataframe.to_csv(output_file, index=False)\n",
    "    logging.info(f\"数据成功保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    collected_data,nc = load_clean_save_data(input_dir,output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------- 拟合模块 -------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- 拟合模块 -------------------------- #\n",
    "# src/fitting.py\n",
    "\n",
    "class FittingModule:\n",
    "    \"\"\"指数拟合模块\"\"\"\n",
    "    \n",
    "    def __init__(self, logger=None, config=None):\n",
    "        \"\"\"\n",
    "        初始化拟合模块。\n",
    "        \n",
    "        参数:\n",
    "        - logger (logging.Logger): 可选的日志记录器。如果未提供，将使用根日志记录器。\n",
    "        - config (dict): 配置字典，包含拟合参数。\n",
    "        \"\"\"\n",
    "        self.logger = logger or logging.getLogger(__name__)\n",
    "        self.config = config or {}\n",
    "        self.initial_fit_points = self.config.get('initial_fit_points', 10)\n",
    "        self.min_r_squared = self.config.get('min_r_squared', 0.95)\n",
    "        self.max_iterations = self.config.get('max_iterations', 10)\n",
    "        self.output_dir = self.config.get('output_dir', './output')\n",
    "        \n",
    "        logging.info(\"拟合模块初始化完成, output_dir: %s\", self.output_dir)\n",
    "        \n",
    "        # 创建输出目录的子文件夹\n",
    "        self.init_output_dir = os.path.join(self.output_dir, \"0_initial\")\n",
    "        os.makedirs(self.init_output_dir, exist_ok=True)\n",
    "        self.fit_output_dir = os.path.join(self.output_dir, \"1_exponential_fit\")\n",
    "        os.makedirs(self.fit_output_dir, exist_ok=True)\n",
    "    \n",
    "    def exponential_func(self, t, a, b, c, d):\n",
    "        \"\"\"\n",
    "        定义拟合函数：a * exp(b * (t - c)) + d * t + e\n",
    "        \n",
    "        参数:\n",
    "        - t (array-like): 自变量数据。\n",
    "        - a, b, c, d, e (float): 拟合参数。\n",
    "        \n",
    "        返回:\n",
    "        - array-like: 函数值。\n",
    "        \"\"\"\n",
    "        return a * np.exp(b * (t - c)**(3/2)) + d\n",
    "    \n",
    "    def line_exponential_func(self, t, a, b ,c, d, e, f):\n",
    "        '''\n",
    "        定义分段拟合函数：a * exp(b * (t - c)) + d * t\n",
    "        \n",
    "        参数:\n",
    "        - t (array-like): 自变量数据。\n",
    "        - a, b, c, d, e (float): 拟合参数。\n",
    "        \n",
    "        返回:\n",
    "        - array-like: 函数值。\n",
    "        '''\n",
    "        # 找到nb.abs(t-c)最小值的位置\n",
    "        index = np.argmin(np.abs(t-f))\n",
    "        N1 = e*np.ones_like(t[:index])\n",
    "        N2 = a * np.exp(b * (t[index:] - c)**(3/2)) + d\n",
    "        return np.concatenate((N1,N2))\n",
    "    \n",
    "\n",
    "    def find_line(self, new_popt, t_fit, N_fit, new_start_idx, new_end_idx):\n",
    "        \"\"\"\n",
    "        优化拟合参数c和e，使得拟合的R²尽可能大。\n",
    "        \n",
    "        参数:\n",
    "        - new_popt (list): 当前拟合的参数，包含 a, b, c, d, e。\n",
    "        - new_r_squared (float): 当前拟合的 R² 值。\n",
    "        - new_start_idx (int): 拟合数据的起始索引。\n",
    "        - new_end_idx (int): 拟合数据的结束索引。\n",
    "        \n",
    "        返回:\n",
    "        - final_popt (list): 优化后的拟合参数，包含 a, b, c, d, e。\n",
    "        - final_r_squared (float): 优化后的 R² 值。\n",
    "        \"\"\"\n",
    "        \n",
    "        # 提取当前拟合参数\n",
    "        a, b, c, d= new_popt\n",
    "        e = 0.9999*d\n",
    "        f = 1.0001*c\n",
    "        # 优化目标函数，最大化 R²\n",
    "        def objective(params):\n",
    "            # 固定 a, b, c, d, 只优化 e 和 f\n",
    "            e, f = params\n",
    "            \n",
    "            # 使用 line_exponential_func 进行拟合\n",
    "            fitted_N = self.line_exponential_func(t_fit, a, b, c, d, e, f)\n",
    "            index = np.argmin(np.abs(t_fit-f))\n",
    "            gap = np.abs(a * np.exp(b * (t_fit[index] - c)**(3/2)) + d - e)\n",
    "\n",
    "            # 计算拟合残差\n",
    "            residuals = fitted_N - N_fit\n",
    "            ss_res = np.sum(residuals**2)\n",
    "            ss_tot = np.sum((N_fit - np.mean(N_fit))**2)\n",
    "            \n",
    "            # 计算 R² 值\n",
    "            r_squared = 1 - (ss_res / ss_tot)\n",
    "            \n",
    "            self.logger.info(f\"优化中：参数={params}, R²={r_squared:.4f}\")\n",
    "            \n",
    "            # 目标是最大化 R²，最小化负的 R²\n",
    "            return -r_squared*(f/t_fit[-1])*(1-gap/(np.max(N_fit)-np.min(N_fit))) if r_squared >= self.min_r_squared else 0\n",
    "\n",
    "        # 使用 minimize 优化 c 和 e\n",
    "        result = minimize(objective, x0=[e, f], bounds=[(0, np.inf), (t_fit[0], t_fit[-1])], method='L-BFGS-B')\n",
    "        \n",
    "        # 获取优化后的参数\n",
    "        optimized_e, optimized_f = result.x\n",
    "        final_popt = [a, b, c, d, optimized_e, optimized_f]\n",
    "\n",
    "        # 计算最终的 R²\n",
    "        fitted_N = self.line_exponential_func(t_fit, *final_popt)\n",
    "\n",
    "        # 计算拟合残差\n",
    "        residuals = fitted_N - N_fit\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        ss_tot = np.sum((N_fit - np.mean(N_fit))**2)\n",
    "        final_r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "        self.logger.debug(f\"最后拟合结果：参数={final_popt}, R²={final_r_squared:.4f}\")\n",
    "        return final_popt, final_r_squared, np.argmin(np.abs(t_fit-f)), new_end_idx\n",
    "\n",
    "\n",
    "    def fit_dynamic(self, t, N, index, initial_fit_points=None, min_r_squared=None, max_iterations=None):\n",
    "        \"\"\"\n",
    "        动态进行指数拟合，逐步扩大拟合范围直到满足置信度要求或达到最大迭代次数。\n",
    "        \n",
    "        参数:\n",
    "        - t (pd.Series or np.ndarray): 时间序列。\n",
    "        - N (pd.Series or np.ndarray): 数据序列。\n",
    "        - initial_fit_points (int, optional): 初始拟合的数据点数。\n",
    "        - min_r_squared (float, optional): 最小的 R² 值要求。\n",
    "        - max_iterations (int, optional): 最大拟合尝试次数，防止无限循环。\n",
    "        \n",
    "        返回:\n",
    "        - dict or None: 包含拟合参数和 R² 值的字典，若拟合失败则返回 None。\n",
    "        \"\"\"\n",
    "        # 使用传入的参数或配置中的参数\n",
    "        initial_fit_points = initial_fit_points or self.initial_fit_points\n",
    "        min_r_squared = min_r_squared or self.min_r_squared\n",
    "        max_iterations = max_iterations or self.max_iterations\n",
    "        \n",
    "        self.logger.info(\"开始动态指数拟合...\\n - 初始拟合点数: %d  - 最小 R²: %.2f  - 最大迭代次数: %d\", initial_fit_points, min_r_squared, max_iterations)\n",
    "        \n",
    "        # 确保输入为 numpy 数组\n",
    "        t = np.array(t)\n",
    "        N = np.array(N)\n",
    "        self.plot_source(t, N, index)\n",
    "\n",
    "        total_points = len(t)\n",
    "        if total_points < initial_fit_points:\n",
    "            self.logger.error(f\"数据点不足，无法进行拟合。需要至少 {initial_fit_points} 个点，当前有 {total_points} 个点。\")\n",
    "            return None\n",
    "        \n",
    "        # 初始拟合范围：最后 initial_fit_points 个点\n",
    "        start_idx = total_points - initial_fit_points\n",
    "        end_idx = total_points\n",
    "        new_start_idx = start_idx\n",
    "        new_end_idx = end_idx\n",
    "\n",
    "        iteration = 0\n",
    "        new_popt = None\n",
    "        new_r_squared = -np.inf\n",
    "        new_t_fit=t\n",
    "\n",
    "        \n",
    "        while iteration < max_iterations and start_idx >= 0:\n",
    "            self.logger.debug(f\"拟合迭代 {iteration + 1}: 使用数据点 {start_idx} 到 {end_idx}（共 {end_idx - start_idx} 个点）\")\n",
    "            t_fit = t[start_idx:end_idx]\n",
    "            N_fit = N[start_idx:end_idx]\n",
    "            \n",
    "            try:\n",
    "                # 初始参数猜测\n",
    "                initial_params = [1.0, 0.1, t_fit[0], np.min(N_fit)]\n",
    "                # 参数边界\n",
    "                bounds = ([0, 0, 0, 0], [np.inf, np.inf, np.min(t_fit), np.max(N_fit)])\n",
    "\n",
    "                # # 初始参数猜测\n",
    "                # initial_params = [np.min(N_fit), 1, 1, np.min(t_fit)]\n",
    "                # # 参数边界\n",
    "                # bounds = ([0, 0, 0, 0], [np.max(N_fit), np.inf, np.inf, np.min(t_fit)])\n",
    "                \n",
    "                popt, pcov = curve_fit(\n",
    "                    self.exponential_func, \n",
    "                    t_fit, \n",
    "                    N_fit, \n",
    "                    p0=initial_params, \n",
    "                    bounds=bounds, \n",
    "                    maxfev=10000\n",
    "                )\n",
    "                \n",
    "                # 计算拟合结果\n",
    "                residuals = N_fit - self.exponential_func(t_fit, *popt)\n",
    "                ss_res = np.sum(residuals**2)\n",
    "                ss_tot = np.sum((N_fit - np.mean(N_fit))**2)\n",
    "                r_squared = 1 - (ss_res / ss_tot)\n",
    "                \n",
    "                self.logger.debug(f\"拟合结果：参数={popt}, R²={r_squared:.4f}\")\n",
    "                \n",
    "                # 检查 R² 是否达到要求\n",
    "                if r_squared < min_r_squared:\n",
    "                    self.logger.info(f\"拟合成功，R²={new_r_squared:.4f}，使用数据点 {new_start_idx} 到 {new_end_idx}\")\n",
    "                    final_popt, final_r_squared, final_start_idx, final_end_idx= self.find_line(new_popt, t_fit, N_fit, new_start_idx, new_end_idx)\n",
    "                    # 绘制并保存拟合结果\n",
    "                    self.plot_fit(t_fit, N_fit, new_t_fit, index, final_popt, final_r_squared, final_start_idx, final_end_idx)\n",
    "                    return {\n",
    "                        \"index\": index,\n",
    "                        'params': final_popt,\n",
    "                        'r_squared': final_r_squared,\n",
    "                        'start_idx': final_start_idx,\n",
    "                        'end_idx': final_end_idx\n",
    "                    }\n",
    "                else:\n",
    "                    # 保存最佳拟合结果\n",
    "                    new_popt = popt\n",
    "                    new_r_squared = r_squared\n",
    "                    new_start_idx = start_idx\n",
    "                    new_end_idx = end_idx\n",
    "                    new_t_fit = t_fit\n",
    "                \n",
    "                # 扩大拟合范围\n",
    "                start_idx = max(0, start_idx - initial_fit_points)\n",
    "                iteration += 1\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                self.logger.warning(f\"拟合失败在数据点 {start_idx} 到 {end_idx}: {e}\")\n",
    "                # 扩大拟合范围继续尝试\n",
    "                start_idx = max(0, start_idx - initial_fit_points)\n",
    "                iteration += 1\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"拟合过程中发生未预料的错误: {e}\")\n",
    "                return None\n",
    "        \n",
    "        # 如果未达到要求，返回最佳结果并绘图\n",
    "        if new_r_squared >= min_r_squared:\n",
    "            self.logger.info(f\"拟合成功，R²={r_squared:.4f}，使用数据点 {start_idx} 到 {end_idx}\")\n",
    "            final_popt, final_r_squared, final_start_idx, final_end_idx= self.find_line(new_popt, t_fit, N_fit, new_start_idx, new_end_idx)\n",
    "            # 绘制并保存拟合结果\n",
    "            self.plot_fit(t_fit, N_fit, new_t_fit, index, final_popt, final_r_squared, final_start_idx, final_end_idx)\n",
    "            return {\n",
    "                \"index\": index,\n",
    "                'params': final_popt,\n",
    "                'r_squared': final_r_squared,\n",
    "                'start_idx': final_start_idx,\n",
    "                'end_idx': final_end_idx\n",
    "            }\n",
    "        elif iteration >= max_iterations:\n",
    "            self.logger.error(\"超过限制次数，所有拟合尝试均失败。\")\n",
    "            return None\n",
    "    \n",
    "    def plot_fit(self, t_fit, N_fit, new_t_fit, index, popt, r_squared, start_idx, end_idx):\n",
    "        \"\"\"\n",
    "        绘制拟合曲线并保存图像。\n",
    "        \n",
    "        参数:\n",
    "        - t_fit (np.ndarray): 拟合的时间序列。\n",
    "        - N_fit (np.ndarray): 拟合的数据序列。\n",
    "        - popt (list): 拟合参数。\n",
    "        - r_squared (float): 拟合的 R² 值。\n",
    "        - start_idx (int): 拟合的起始索引。\n",
    "        - end_idx (int): 拟合的结束索引。\n",
    "        \n",
    "        返回:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(t_fit, N_fit, label='Data', color='blue')\n",
    "        fitted_N = self.line_exponential_func(new_t_fit, *popt)\n",
    "        plt.plot(new_t_fit, fitted_N, label=f'Fit (R²={r_squared:.4f})', color='red')\n",
    "        plt.vlines(popt[-1], np.min(N_fit), np.max(N_fit), colors='green', linestyles='dashed', label='Increase Parse')\n",
    "        plt.title(f'N{index} Exponential Fit: Data points {start_idx} to {end_idx}')\n",
    "        plt.xlabel('Time(ps)')\n",
    "        plt.ylabel('N')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # 定义保存路径\n",
    "        plot_filename = f\"1_exponential_fit\\{index}-{start_idx}_{end_idx}.png\"\n",
    "        print(plot_filename)\n",
    "        plot_path = os.path.join(self.output_dir, plot_filename)\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        self.logger.info(f\"拟合图已保存到 {plot_path}\")\n",
    "\n",
    "    def plot_source(self, t, N, index):\n",
    "        \"\"\"\n",
    "        绘制原始数据并保存图像。\n",
    "        \n",
    "        参数:\n",
    "        - t (np.ndarray): 时间序列。\n",
    "        - N (np.ndarray): 数据序列。\n",
    "        - index (int): 数据索引。\n",
    "        \n",
    "        返回:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.plot(t, N, \"-o\", label='Data', color='blue')\n",
    "        plt.title(f'N{index} Source Data')\n",
    "        plt.xlabel('Time(ps)')\n",
    "        plt.ylabel('N')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # 定义保存路径\n",
    "        plot_filename = f\"0_initial\\{index}-source.png\"\n",
    "        plot_path = os.path.join(self.output_dir, plot_filename)\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        self.logger.info(f\"原始数据图已保存到 {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 配置日志记录（如上）\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.StreamHandler()           # 同时输出到控制台\n",
    "        ]\n",
    "    )\n",
    "    # 初始化 FittingModule 实例\n",
    "    config = load_config(\"C:\\\\Users\\\\husky\\\\Desktop\\\\peek-find\\\\PEM4Fe\\\\data\\\\config.yaml\")  # 或者 \"config.json\"\n",
    "    fitting_module = FittingModule(config=config)\n",
    "    logging.info(\"FittingModule 实例已初始化。\")\n",
    "\n",
    "    # 提取时间序列和数据序列\n",
    "    t = collected_data['t']\n",
    "\n",
    "    for col in collected_data.columns[1:]:  # 从第2列到结束\n",
    "        N = collected_data[col]  # 确保 'N' 列名正确\n",
    "        index = str(col)\n",
    "        logging.info(f\"开始执行动态指数拟合：{col}...\")\n",
    "        # 去除无效值\n",
    "        t_clean, N_clean = clean_data(t, N)\n",
    "        fit_result = fitting_module.fit_dynamic(t_clean, N_clean, index)\n",
    "        \n",
    "        if fit_result:\n",
    "            # print(fit_result)\n",
    "            params = fit_result['params']\n",
    "            r_squared = fit_result['r_squared']\n",
    "            logging.info(f\"拟合成功。R² = {r_squared:.4f}\")\n",
    "            logging.info(f\"拟合参数: a = {params[0]:.4f}, b = {params[1]:.4f}, c = {params[2]:.4f}, d = {params[3]:.4f}, e = {params[4]:.4f}, f = {params[5]:.4f}\")\n",
    "        else:\n",
    "            logging.error(\"拟合失败。\")\n",
    "        logging.info(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------- 峰值检测模块 -------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "class PeakDetectionModule:\n",
    "    \"\"\"峰值检测模块\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None, logger=None):\n",
    "        \"\"\"\n",
    "        初始化拟合模块。\n",
    "        \n",
    "        参数:\n",
    "        - config (dict): 配置字典，包含拟合参数。\n",
    "        - logger (logging.Logger): 可选的日志记录器。如果未提供，将使用根日志记录器。\n",
    "        \"\"\"\n",
    "        self.logger = logger or logging.getLogger(__name__)\n",
    "        self.config = config or {}\n",
    "        \n",
    "        # 提取配置中的参数\n",
    "        self.Nsc = self.config.get('Nsc', 1000)\n",
    "        self.N0 = self.config.get('N0', 0)\n",
    "\n",
    "        self.width_threshold = self.config.get('width_threshold', 5)\n",
    "        self.plateau_size = self.config.get('plateau_size', 1)\n",
    "        self.num_density = self.config.get('num_density', -1)\n",
    "\n",
    "        self.window_size = self.config.get('window_size', 5)\n",
    "        self.tolerance_ratio = self.config.get('tolerance_ratio', 0.1)\n",
    "\n",
    "        self.output_dir = self.config.get('output_dir', './output')\n",
    "        \n",
    "        # 创建输出目录的子文件夹\n",
    "        self.plot_output_dir = os.path.join(self.output_dir, \"2_peak_detection_plots\")\n",
    "        os.makedirs(self.plot_output_dir, exist_ok=True)\n",
    "        \n",
    "        self.logger.info(\"峰值检测模块初始化完成, output_dir: %s\", self.output_dir)\n",
    "\n",
    "    def detect_peaks(self, N, column_name, Nsc=None, width_threshold=None):\n",
    "        \"\"\"\n",
    "        检测数据中的峰值。\n",
    "        \n",
    "        参数:\n",
    "        - N (array-like): 数据序列。\n",
    "        - Nsc (float, optional): 峰值的阈值。若未提供，则使用 config 中的 Nsc。\n",
    "        - width_threshold (int, optional): 峰的最小宽度（数据点数量）。若未提供，则使用 config 中的值。\n",
    "        \n",
    "        返回:\n",
    "        - list: 包含每个峰的信息的列表，每个元素是一个字典，包含峰的范围、宽度和均值。\n",
    "        \"\"\"\n",
    "        Nsc = Nsc or self.Nsc\n",
    "        width_threshold = width_threshold or self.width_threshold\n",
    "        plateau_size=self.plateau_size\n",
    "\n",
    "        self.logger.info(\"开始检测峰值...\")\n",
    "\n",
    "        # 寻找峰值\n",
    "        peaks, properties = find_peaks(N, height=self.Nsc, width=width_threshold, plateau_size=plateau_size)\n",
    "        peaks_info = []\n",
    "\n",
    "        for i, peak in enumerate(peaks):\n",
    "            width = int(properties['widths'][i])\n",
    "            if width >= width_threshold:\n",
    "                left_ips = int(properties['left_ips'][i])\n",
    "                right_ips = int(properties['right_ips'][i])\n",
    "                peak_range = range(left_ips, right_ips + 1)\n",
    "                N_peak = N[peak_range]\n",
    "                peak_mean = np.mean(N_peak)\n",
    "                self.logger.debug(f\"峰值 {peak}: left={left_ips}, right={right_ips}, width={width}, mean={peak_mean:.4f}\")\n",
    "                if peak_mean > Nsc:\n",
    "                    peaks_info.append({\n",
    "                        'column': column_name,\n",
    "                        'peak_index': peak,\n",
    "                        'left': left_ips,\n",
    "                        'right': right_ips,\n",
    "                        'width': width,\n",
    "                        'mean': peak_mean,\n",
    "                        'indices': peak_range\n",
    "                    })\n",
    "\n",
    "        self.logger.info(f\"检测到 {len(peaks_info)} 个峰值。\")\n",
    "        return peaks_info\n",
    "\n",
    "    def plot_peaks(self, ax, t, N, peaks_info, column_name):\n",
    "        \"\"\"\n",
    "        绘制峰值检测结果，并保存图像。\n",
    "        \n",
    "        参数:\n",
    "        - ax (matplotlib.axes._axes.Axes): 子图对象，用于绘制数据。\n",
    "        - t (array-like): 时间序列。\n",
    "        - N (array-like): 数据序列。\n",
    "        - peaks_info (list): 包含峰值信息的列表。\n",
    "        - column_name (str): 数据列名。\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"绘制峰值检测图 for {column_name}...\")\n",
    "        \n",
    "        ax.plot(t, N, label=\"Original Data\", color=\"blue\")\n",
    "        \n",
    "        # 绘制峰值\n",
    "        for peak_info in peaks_info:\n",
    "            peak_t = t[peak_info['indices']]\n",
    "            peak_N = N[peak_info['indices']]\n",
    "            ax.plot(peak_t, peak_N, color=\"red\")\n",
    "            peak_left = peak_info['left']\n",
    "            peak_right = peak_info['right']\n",
    "            ax.fill_between(t[peak_left:peak_right + 1], N[peak_left:peak_right + 1], self.N0, alpha=0.5)\n",
    "            ax.hlines(peak_info['mean'], t[peak_left], t[peak_right], color=\"r\", linestyle=\"--\")\n",
    "        \n",
    "\n",
    "    def plot_density_analysis(self, ax, ax_twin, N, peaks_info, column_name, num_density=-1):\n",
    "        \"\"\"\n",
    "        绘制右侧子图，显示峰值密集度分析和直方图。\n",
    "        \n",
    "        参数:\n",
    "        - ax (matplotlib.axes._axes.Axes): 子图对象，用于绘制密集区域分析。\n",
    "        - ax_twin (matplotlib.axes._axes.Axes): 共享 y 轴的子图对象，用于绘制直方图和密度曲线。\n",
    "        - N (pd.Series): 数据序列。\n",
    "        - Nsc (float): 峰值的阈值。\n",
    "        - N0 (float): 数据的基准值。\n",
    "        - column_name (str): 数据列名。\n",
    "        - directory_path (str): 保存结果的目录路径。\n",
    "        - num_density (int): 检测的密集区域峰值数量，默认 -1 表示检测所有峰值。\n",
    "        \n",
    "        返回:\n",
    "        - list: 包含密集区域峰值信息的列表。\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"开始绘制密集区域分析图 for {column_name}...\")\n",
    "        widths = [info['width'] for info in peaks_info]\n",
    "        means = [info['mean'] for info in peaks_info]\n",
    "\n",
    "        Nsc, N0 = self.Nsc, self.N0\n",
    "        \n",
    "        if means:\n",
    "            N_cut = np.max(means)\n",
    "        else:\n",
    "            N_cut = Nsc\n",
    "        ymax = N.max()\n",
    "        \n",
    "        if widths and means:\n",
    "            ax.scatter(widths, means, color='blue')\n",
    "\n",
    "        # 设置 ax 的 y 轴范围\n",
    "        ax.set_ylim(Nsc, ymax)\n",
    "        ax.set_title(f\"{column_name} Density Analysis\")\n",
    "        \n",
    "        # 绘制原始直方图\n",
    "        ax_twin.hist(N[N >= N_cut], bins=2000, orientation='horizontal', alpha=0.6, color='gray', label='(N | N >= Nsc) Distribution')\n",
    "\n",
    "        # 计算直方图数据\n",
    "        N_smooth = N[(N >= N_cut) & (N <= ymax)]\n",
    "        counts, bin_edges = np.histogram(N_smooth, bins=1000)\n",
    "        bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "        # 对直方图数据进行高斯平滑\n",
    "        sigma = 0.5  # 控制平滑强度\n",
    "        smoothed_counts = gaussian_filter1d(counts[:np.nonzero(counts)[0][-1] + 1], sigma=sigma)\n",
    "\n",
    "        # 绘制高斯平滑曲线\n",
    "        ax_twin.plot(smoothed_counts, bin_centers, color='red', linestyle='-', linewidth=0.5, label='Smoothed Density', alpha=0.8)\n",
    "        \n",
    "        # 峰值检测\n",
    "        peak_indices, properties = find_peaks(smoothed_counts, height=0, plateau_size=0, width=0)\n",
    "\n",
    "        density_info = []\n",
    "\n",
    "        if peak_indices.size != 0:\n",
    "            up = np.array(properties[\"plateau_sizes\"])\n",
    "            down = np.array(properties[\"width_heights\"])\n",
    "            high = np.array(properties[\"peak_heights\"])\n",
    "            sorted_peak_indices = peak_indices[np.argsort((up + down) * 0.5 * high)][::-1]\n",
    "\n",
    "            # 确保 num_density 不超过 sorted_peak_indices 的长度\n",
    "            num_density = min(num_density, len(sorted_peak_indices))\n",
    "\n",
    "            density_place = bin_centers[sorted_peak_indices[:num_density]]\n",
    "            ax_twin.scatter(smoothed_counts[sorted_peak_indices[:num_density]], bin_centers[sorted_peak_indices[:num_density]], \n",
    "                            color='green', label='Density Peaks', marker='x')\n",
    "\n",
    "            for i, N_forecast in enumerate(density_place):\n",
    "                density_info.append({\n",
    "                    'peak_index': i,\n",
    "                    'forecast': N_forecast,\n",
    "                    'smoothed_count': smoothed_counts[sorted_peak_indices[i]],\n",
    "                })\n",
    "        \n",
    "        # 添加图例\n",
    "        ax_twin.legend(fontsize=8)\n",
    "        \n",
    "        return density_info\n",
    "    \n",
    "    def find_nearest_interval(self, ax, t, N, N_forecast, window_size=None, tolerance_ratio=None):\n",
    "        \"\"\"\n",
    "        寻找 N 中接近 N_forecast 的所有区段，返回多个区段的起止点。\n",
    "\n",
    "        参数：\n",
    "        - ax (matplotlib.axes._axes.Axes): 绘图的子图，用于标记结果。\n",
    "        - t (array-like): 时间序列。\n",
    "        - N (array-like): 实际数据数组。\n",
    "        - N_forecast (float): 目标值，寻找接近该值的多个区段。\n",
    "        - window_size (int): 平滑窗口大小，默认为 5。\n",
    "        - tolerance_ratio (float): 容差范围比例，默认为 0.01。\n",
    "\n",
    "        返回：\n",
    "        - list of dict: 每个区段的信息，包括起止点、均值等。\n",
    "        \"\"\"\n",
    "        window_size = self.window_size or window_size\n",
    "        tolerance_ratio = self.tolerance_ratio or tolerance_ratio\n",
    "\n",
    "        # Step 0: 归一化\n",
    "        N = np.array(N)\n",
    "        min_N = N.min()\n",
    "        max_N = N.max()\n",
    "        Nsc = (self.Nsc - N.min()) / (N.max() - N.min())  # 归一化 Nsc\n",
    "        N_forecast = (N_forecast - N.min()) / (N.max() - N.min())  # 归一化 N_forecast\n",
    "        N = (N - N.min()) / (N.max() - N.min())\n",
    "\n",
    "        tolerance = tolerance_ratio * N_forecast  # 容差范围\n",
    "\n",
    "        # Step 1: 平滑数据\n",
    "        N_smooth = pd.Series(N).rolling(window=window_size, center=True).mean().fillna(0).to_numpy()\n",
    "\n",
    "        # Step 2: 过滤小于 Nsc 的值\n",
    "        valid_indices = N > Nsc\n",
    "        N_smooth[~valid_indices] = 0\n",
    "\n",
    "        # Step 3: 找到所有满足条件的索引\n",
    "        valid_mask = np.abs(N_smooth - N_forecast) <= tolerance\n",
    "        valid_indices = np.where(valid_mask)[0]  # 获取满足条件的索引\n",
    "        self.logger.debug(f\"目标 {N_forecast*(max_N-min_N)+min_N}，找到 {len(valid_indices)} 个满足条件的索引\")\n",
    "\n",
    "        if len(valid_indices) == 0:\n",
    "            self.logger.warning(f\"未找到满足条件的区段\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Step 4: 合并连续的有效索引为区段\n",
    "        intervals = []\n",
    "        if len(valid_indices) > 0:\n",
    "            start_idx = valid_indices[0]\n",
    "            for i in range(1, len(valid_indices)):\n",
    "                # 判断是否连续\n",
    "                if valid_indices[i] != valid_indices[i - 1] + 1:\n",
    "                    # 新区段开始\n",
    "                    end_idx = valid_indices[i - 1]\n",
    "                    intervals.append((start_idx, end_idx))\n",
    "                    start_idx = valid_indices[i]\n",
    "            # 添加最后一个区段\n",
    "            intervals.append((start_idx, valid_indices[-1]))\n",
    "\n",
    "        intervals.sort(key=lambda x: x[0])\n",
    "        merged_intervals = []\n",
    "        for current in intervals:\n",
    "            # 如果 merged_intervals 为空，或者当前区间与最后一个区间不重叠，则加入结果\n",
    "            if not merged_intervals or merged_intervals[-1][1] < current[0]:\n",
    "                merged_intervals.append(current)\n",
    "            else:\n",
    "                # 否则，合并当前区间和最后一个区间\n",
    "                merged_intervals[-1] = (merged_intervals[-1][0], max(merged_intervals[-1][1], current[1]))\n",
    "\n",
    "        # Step 5: 计算每个区段的均值，并返回信息\n",
    "        interval_info = []\n",
    "        for start_idx, end_idx in merged_intervals:\n",
    "            start_idx = start_idx - window_size // 2\n",
    "            end_idx = end_idx + window_size // 2\n",
    "            interval_mean = np.mean(N[start_idx:end_idx + 1]*(max_N - min_N) + min_N)\n",
    "            interval_info.append({\n",
    "                'start_index': start_idx,\n",
    "                'end_index': end_idx,\n",
    "                'mean': interval_mean,\n",
    "                'std': np.std(N[start_idx:end_idx + 1]*(max_N - min_N) + min_N)\n",
    "            })\n",
    "            self.logger.debug(f\"区段 [{start_idx}-{end_idx}] 的均值为 {interval_mean:.4f}, 标准差为 {np.std(N[start_idx:end_idx + 1]):.4f}\")\n",
    "            # 可视化标记区段\n",
    "            ax.fill_between(t[start_idx:end_idx + 1], N[start_idx:end_idx + 1], alpha=0.3, color='green')\n",
    "        \n",
    "        return start_idx, end_idx, interval_mean\n",
    "\n",
    "\n",
    "    def plot_plateaus(self, ax, t, N, density_info, column_name):\n",
    "        '''\n",
    "        绘制平台检测结果，并保存图像。\n",
    "\n",
    "        参数:\n",
    "        - ax (matplotlib.axes._axes.Axes): 子图对象，用于绘制数据。\n",
    "        - t (array-like): 时间序列。\n",
    "        - N (array-like): 数据序列。\n",
    "        - peaks_info (list): 包含峰值信息的列表。\n",
    "        - column_name (str): 数据列名。\n",
    "\n",
    "        返回:\n",
    "        - list: 包含平台信息的列表。\n",
    "        '''\n",
    "        self.logger.info(f\"绘制平台检测图 for {column_name}...\")\n",
    "\n",
    "        plateau_info = []\n",
    "        for i, info in enumerate(density_info):\n",
    "            N_forecast = info['forecast']\n",
    "            start_index, end_index, mean= self.find_nearest_interval(ax, t, N, N_forecast)\n",
    "            if start_index is None or end_index is None:\n",
    "                continue\n",
    "            plateau_t = t[start_index:end_index + 1]\n",
    "            plateau_N = N[start_index:end_index + 1]\n",
    "            ax.plot(plateau_t, plateau_N, color=\"green\")\n",
    "            ax.fill_between(plateau_t, plateau_N, np.max(N), alpha=0.3, color='black')\n",
    "            plateau_info.append({\n",
    "                \"column\": column_name,\n",
    "                \"density_index\": i,\n",
    "                \"left\": start_index,\n",
    "                \"right\": end_index,\n",
    "                \"width\": end_index - start_index + 1,\n",
    "                \"mean\": N_forecast,\n",
    "                \"indices\": range(start_index, end_index + 1)\n",
    "            })\n",
    "\n",
    "        ax.set_title(f\"Peak Detection for {column_name}\")\n",
    "        ax.set_ylim(self.N0*0.9, np.max(N)*1.1)\n",
    "        ax.set_xlabel(\"Time(ps)\")\n",
    "        ax.set_ylabel(\"N\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "        return plateau_info\n",
    "\n",
    "\n",
    "    def plot_analysis_figure(self, t, N, peaks_info, column_name, num_density=None):\n",
    "        \"\"\"\n",
    "        绘制分析图，有两个子图。\n",
    "\n",
    "        参数:\n",
    "        - t (pd.Series): 时间序列。\n",
    "        - N (pd.Series): 数据序列。\n",
    "        - peaks_info (list): 峰值信息列表。\n",
    "        - Nsc (float): Nsc 值。\n",
    "        - N0 (float): 数据的基准值。\n",
    "        - column_name (str): 数据列名。\n",
    "        - directory_path (str): 保存结果的目录路径。\n",
    "        - num_density (int): 检测的密集区域峰值数量，默认 -1 表示检测所有峰值。\n",
    "        \n",
    "        返回:\n",
    "        - tuple: (density_info, N_star)\n",
    "        \"\"\"\n",
    "        N0 = self.N0 or N.min()\n",
    "        Nsc = self.Nsc or N.mean()\n",
    "        num_density = self.num_density or -1\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, gridspec_kw={'width_ratios': [5, 1]}, figsize=(14, 6), dpi=300)\n",
    "        ax2_twin = ax2.twiny()  # 为右侧子图设置共享轴\n",
    "\n",
    "        # 绘制左侧子图 ax1\n",
    "        self.plot_peaks(ax1, t, N, peaks_info, column_name)\n",
    "\n",
    "        # 绘制右侧子图 ax2\n",
    "        density_info = self.plot_density_analysis(ax2, ax2_twin, N, peaks_info, column_name, num_density)\n",
    "\n",
    "        plateaus_info = self.plot_plateaus(ax1, t, N, density_info, column_name)\n",
    "\n",
    "        # 设置整体标题\n",
    "        plt.suptitle(f'{column_name} Analysis')\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.88)\n",
    "        plot_file = os.path.join(self.plot_output_dir, f\"{column_name}_peaks.png\")\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "        self.logger.info(f\"峰值图已保存到 {plot_file}\")\n",
    "\n",
    "        return plateaus_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 配置日志记录（如上）\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.StreamHandler()           # 同时输出到控制台\n",
    "        ]\n",
    "    )\n",
    "    # 初始化 FittingModule 实例\n",
    "    config = load_config(\"C:\\\\Users\\\\husky\\\\Desktop\\\\peek-find\\\\PEM4Fe\\\\data\\\\config.yaml\")  # 或者 \"config.json\"\n",
    "    peak_detection_module = PeakDetectionModule(config=config)\n",
    "    logging.info(\"PeakDetectionModule 实例已初始化。\")\n",
    "\n",
    "    # 加载数据\n",
    "    t = collected_data['t']\n",
    "    Peak = []\n",
    "    Plateaus = []\n",
    "    \n",
    "    # 对每一列数据执行峰值检测\n",
    "    for col in collected_data.columns[1:]:  # 从第2列到结束\n",
    "        N = collected_data[col]  # 确保 'N' 列名正确\n",
    "        index = str(col)\n",
    "        logging.info(f\"开始执行峰值检测：{col}...\")\n",
    "\n",
    "        # 去除无效值\n",
    "        t_clean, N_clean = clean_data(t, N)\n",
    "        \n",
    "        # 执行峰值检测\n",
    "        peaks_info = peak_detection_module.detect_peaks(N_clean,index)\n",
    "        \n",
    "        # 打印检测结果\n",
    "        if peaks_info:\n",
    "            logging.info(f\"检测到 {len(peaks_info)} 个峰值。\")\n",
    "        else:\n",
    "            logging.warning(f\"{col} 没有检测到峰值。\")\n",
    "        \n",
    "        # 绘制并保存峰值检测结果\n",
    "        plateaus_info = peak_detection_module.plot_analysis_figure(t_clean, N_clean, peaks_info, index)\n",
    "\n",
    "        Peak.append(peaks_info)\n",
    "        Plateaus.append(plateaus_info)\n",
    "        logging.info(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------- 可视化模块 -------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MainModule:\n",
    "    \"\"\"主体模块\"\"\"\n",
    "    def __init__(self, config=None):\n",
    "        \"\"\"\n",
    "        初始化主体模块。\n",
    "        \n",
    "        参数:\n",
    "        - config (dict): 配置字典，包含所需参数。\n",
    "        \"\"\"\n",
    "        self.config = config or {}\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.Nsc = self.config.get('Nsc', 1000)\n",
    "        self.N0 = self.config.get('N0', 0)\n",
    "        self.output_dir = self.config.get('output_dir', './output')\n",
    "        self.overall_path = os.path.join(self.output_dir, \"3_overall_analysis_plots\")\n",
    "        os.makedirs(self.overall_path, exist_ok=True)\n",
    "\n",
    "    def save_results_to_csv(self, Peak, Plateaus):\n",
    "        \"\"\"保存分析结果到 CSV 文件\"\"\"\n",
    "        \n",
    "        directory_path = self.overall_path\n",
    "\n",
    "        # 构建 DataFrame 并保存为 CSV 文件\n",
    "        data = []\n",
    "        for peaks_info, plateaus_info in zip(Peak, Plateaus):\n",
    "            # 处理峰值信息\n",
    "            for peak_info in peaks_info:\n",
    "                data.append({\n",
    "                    'Column': peak_info['column'],\n",
    "                    'Peak Index': peak_info['peak_index'],\n",
    "                    'Left': peak_info['left'],\n",
    "                    'Right': peak_info['right'],\n",
    "                    'Mean': peak_info['mean'],\n",
    "                    'Width': peak_info['width'],\n",
    "                    'Choice': False,  \n",
    "                    'PTM': False       \n",
    "                })\n",
    "            \n",
    "            # 处理平台信息\n",
    "            for plateau_info in plateaus_info:\n",
    "                data.append({\n",
    "                    'Column': plateau_info['column'],\n",
    "                    'Peak Index': plateau_info['density_index'],\n",
    "                    'Left': plateau_info['left'],\n",
    "                    'Right': plateau_info['right'],\n",
    "                    'Mean': plateau_info['mean'],\n",
    "                    'Width': plateau_info['width'],\n",
    "                    'Choice': False,  \n",
    "                    'PTM': True       \n",
    "                })\n",
    "        \n",
    "        # 创建 DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # 保存到 CSV 文件\n",
    "        output_file = os.path.join(directory_path, 'N_star_info.csv')\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        self.logger.info(f\"分析结果已保存到 {output_file}\")\n",
    "\n",
    "    def plot_overall_analysis(self, Peak, Plateaus):\n",
    "        \"\"\"\n",
    "        绘制总体分析图\n",
    "\n",
    "        参数:\n",
    "        - peaks_info (list): 峰值信息列表。\n",
    "        - plateaus_info (list): 平台信息列表。\n",
    "        - N_star (float): 总体均值。\n",
    "        \"\"\" \n",
    "        \n",
    "        directory_path = self.overall_path\n",
    "        fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "        values_all = np.array([])\n",
    "        columns = []\n",
    "        colors = []            \n",
    "\n",
    "        # 绘制散点图\n",
    "\n",
    "        for i in range(len(Peak)):\n",
    "            peaks_info = Peak[i]\n",
    "            peek_values = [info['mean'] for info in peaks_info]\n",
    "            plateaus_info = Plateaus[i]\n",
    "            plateaus_values = [info['mean'] for info in plateaus_info]\n",
    "    \n",
    "            if len(peaks_info) != 0:\n",
    "                info=peaks_info[0]\n",
    "            elif len(plateaus_info) != 0:\n",
    "                info=plateaus_info[0]\n",
    "            else:\n",
    "                continue\n",
    "            column_name = info['column']\n",
    "            columns.append(column_name)\n",
    "\n",
    "            values_all = np.concatenate((values_all, peek_values, plateaus_values))\n",
    "            scatter = ax.scatter(i*np.ones_like(peek_values), peek_values, marker='D',s=80)\n",
    "            \n",
    "            colors.append(scatter.get_facecolor()[0])  # 记录颜色\n",
    "            ax.scatter(i*np.ones_like(plateaus_values), plateaus_values, facecolors='none', edgecolors=colors[i], marker='o', s=80)\n",
    "\n",
    "            # 风琴图\n",
    "            values= np.concatenate((np.array(peek_values), np.array(plateaus_values)))\n",
    "            ax.violinplot(values, positions=[i], showmedians=True, showextrema=True)\n",
    "\n",
    "        N_star = np.mean(values_all)\n",
    "        ax.axhline(y=N_star, color='k', linestyle='--', label=f'N* = {N_star:.2f}')\n",
    "\n",
    "        ax.set_xticks(range(len(Peak)))\n",
    "        ax.set_xticklabels(columns, rotation=90)\n",
    "\n",
    "        # 添加图例和标题\n",
    "        ax.set_xlabel('Index')\n",
    "        ax.set_ylabel('Mean Value')\n",
    "        ax.set_title(f'Overall Analysis for {len(Peak)} Columns (N*={N_star}, Nsc={self.Nsc}, N0={self.N0})')\n",
    "        ax.legend()\n",
    "\n",
    "        # 保存图像\n",
    "        plot_file = os.path.join(directory_path, 'Overall_analysis.png')\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "        self.logger.info(f\"总体分析图已保存到 {plot_file}\")\n",
    "\n",
    "    def All_PTM(self,show_output=False):\n",
    "        \n",
    "        log_file = \"application.log\"\n",
    "\n",
    "        # 调用日志设置函数\n",
    "        setup_logging(output_dir=output_dir, log_file=log_file, level=logging.DEBUG)\n",
    "        if show_output:\n",
    "            logging.basicConfig(\n",
    "                level=logging.INFO,\n",
    "                format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "                handlers=[\n",
    "                    logging.StreamHandler()           # 同时输出到控制台\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            logging.basicConfig(\n",
    "                level=logging.INFO,\n",
    "                format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "            )\n",
    "\n",
    "        self.overall_path = os.path.join(self.output_dir, \"3_overall_analysis_plots\")\n",
    "        os.makedirs(self.overall_path, exist_ok=True)\n",
    "\n",
    "        collected_data,nc = load_clean_save_data(input_dir,output_dir)\n",
    "\n",
    "        # 初始化 FittingModule 实例\n",
    "        config = load_config(\"C:\\\\Users\\\\husky\\\\Desktop\\\\peek-find\\\\PEM4Fe\\\\data\\\\config.yaml\")  # 或者 \"config.json\"\n",
    "        peak_detection_module = PeakDetectionModule(config=config)\n",
    "        logging.info(\"PeakDetectionModule 实例已初始化。\")\n",
    "\n",
    "        # 加载数据\n",
    "        t = collected_data['t']\n",
    "        Peak = []\n",
    "        Plateaus = []\n",
    "\n",
    "        fitting_module = FittingModule(config=config)\n",
    "        logging.info(\"FittingModule 实例已初始化。\")\n",
    "\n",
    "        for col in collected_data.columns[1:]:  # 从第2列到结束\n",
    "            N = collected_data[col]  # 确保 'N' 列名正确\n",
    "            index = str(col)\n",
    "            logging.info(f\"\\n开始执行动态指数拟合：{col}...\")\n",
    "            # 去除无效值\n",
    "            t_clean, N_clean = clean_data(t, N)\n",
    "            fit_result = fitting_module.fit_dynamic(t_clean, N_clean, index)\n",
    "            \n",
    "            if fit_result:\n",
    "                # print(fit_result)\n",
    "                params = fit_result['params']\n",
    "                r_squared = fit_result['r_squared']\n",
    "                logging.info(f\"拟合成功。R² = {r_squared:.4f}\")\n",
    "                logging.info(f\"拟合参数: a = {params[0]:.4f}, b = {params[1]:.4f}, c = {params[2]:.4f}, d = {params[3]:.4f}, e = {params[4]:.4f}, f = {params[5]:.4f}\")\n",
    "            else:\n",
    "                logging.error(\"拟合失败。\")\n",
    "            logging.info(\"--------------------------------------------------\")\n",
    "\n",
    "        # 对每一列数据执行峰值检测\n",
    "        for col in collected_data.columns[1:]:  # 从第2列到结束\n",
    "            N = collected_data[col]  # 确保 'N' 列名正确\n",
    "            index = str(col)\n",
    "            logging.info(f\"\\n开始执行峰值检测：{col}...\")\n",
    "\n",
    "            # 去除无效值\n",
    "            t_clean, N_clean = clean_data(t, N)\n",
    "            \n",
    "            # 执行峰值检测\n",
    "            peaks_info = peak_detection_module.detect_peaks(N_clean,index)\n",
    "            \n",
    "            # 打印检测结果\n",
    "            if peaks_info:\n",
    "                logging.info(f\"检测到 {len(peaks_info)} 个峰值。\")\n",
    "            else:\n",
    "                logging.warning(f\"{col} 没有检测到峰值。\")\n",
    "            \n",
    "            # 绘制并保存峰值检测结果\n",
    "            plateaus_info = peak_detection_module.plot_analysis_figure(t_clean, N_clean, peaks_info, index)\n",
    "\n",
    "            Peak.append(peaks_info)\n",
    "            Plateaus.append(plateaus_info)\n",
    "            logging.info(\"--------------------------------------------------\")\n",
    "        \n",
    "        # 保存结果到 CSV 文件\n",
    "        main_module = MainModule(config=config)\n",
    "        main_module.save_results_to_csv(Peak, Plateaus)\n",
    "        main_module.plot_overall_analysis(Peak, Plateaus)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 配置日志记录（如上）\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.StreamHandler()           # 同时输出到控制台\n",
    "        ]\n",
    "    )\n",
    "    # 初始化 FittingModule 实例\n",
    "    config = load_config(\"C:\\\\Users\\\\husky\\\\Desktop\\\\peek-find\\\\PEM4Fe\\\\data\\\\config.yaml\")  # 或者 \"config.json\"\n",
    "    peak_detection_module = PeakDetectionModule(config=config)\n",
    "    logging.info(\"PeakDetectionModule 实例已初始化。\")\n",
    "\n",
    "    # 加载数据\n",
    "    t = collected_data['t']\n",
    "    Peak = []\n",
    "    Plateaus = []\n",
    "    \n",
    "    # 对每一列数据执行峰值检测\n",
    "    for col in collected_data.columns[1:]:  # 从第2列到结束\n",
    "        N = collected_data[col]  # 确保 'N' 列名正确\n",
    "        index = str(col)\n",
    "        logging.info(f\"开始执行峰值检测：{col}...\")\n",
    "\n",
    "        # 去除无效值\n",
    "        t_clean, N_clean = clean_data(t, N)\n",
    "        \n",
    "        # 执行峰值检测\n",
    "        peaks_info = peak_detection_module.detect_peaks(N_clean,index)\n",
    "        \n",
    "        # 打印检测结果\n",
    "        if peaks_info:\n",
    "            logging.info(f\"检测到 {len(peaks_info)} 个峰值。\")\n",
    "        else:\n",
    "            logging.warning(f\"{col} 没有检测到峰值。\")\n",
    "        \n",
    "        # 绘制并保存峰值检测结果\n",
    "        plateaus_info = peak_detection_module.plot_analysis_figure(t_clean, N_clean, peaks_info, index)\n",
    "\n",
    "        Peak.append(peaks_info)\n",
    "        Plateaus.append(plateaus_info)\n",
    "        logging.info(\"--------------------------------------------------\")\n",
    "    \n",
    "    # 保存结果到 CSV 文件\n",
    "    main_module = MainModule(config=config)\n",
    "    data=main_module.save_results_to_csv(Peak, Plateaus)\n",
    "    main_module.plot_overall_analysis(Peak, Plateaus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------- 结果输出模块 -------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PEM4Fe.peak_detection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPEM4Fe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPEM4Fe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfitting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FittingModule\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPEM4Fe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPEM4Fe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeak_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeakDetectionModule\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPEM4Fe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPEM4Fe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MainModule\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(config):\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# --------------------- 解析命令行参数 --------------------- #\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# --------------------- 加载配置文件 --------------------- #\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\husky\\Desktop\\peek-find\\PEM4Fe\\src\\PEM4Fe\\output.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPEM4Fe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeak_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeakDetectionModule\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPEM4Fe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfitting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FittingModule\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPEM4Fe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_processing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_clean_save_data, clean_data\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PEM4Fe.peak_detection'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "from PEM4Fe.src.PEM4Fe.config import load_config, setup_logging\n",
    "from PEM4Fe.src.PEM4Fe.data_processing import clean_data, load_clean_save_data\n",
    "from PEM4Fe.src.PEM4Fe.fitting import FittingModule\n",
    "from PEM4Fe.src.PEM4Fe.peak_detection import PeakDetectionModule\n",
    "from PEM4Fe.src.PEM4Fe.output import MainModule\n",
    "\n",
    "def main(config):\n",
    "\n",
    "    # --------------------- 解析命令行参数 --------------------- #\n",
    "    # parser = argparse.ArgumentParser(description=\"PEM4Fe: Data Analysis Tool\")\n",
    "    # parser.add_argument(\n",
    "    #     \"--config\",\n",
    "    #     type=str,\n",
    "    #     required=True,\n",
    "    #     help=\"Path to the configuration file (YAML or JSON).\",\n",
    "    # )\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    # --------------------- 加载配置文件 --------------------- #\n",
    "    try:\n",
    "        config = load_config(config)\n",
    "        logging.info(f\"成功加载配置文件：{config}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"加载配置文件失败：{e}\")\n",
    "        return\n",
    "\n",
    "    # --------------------- 设置日志 --------------------- #\n",
    "    log_file = \"application.log\"\n",
    "    setup_logging(output_dir=config.get(\"output_dir\", \"./output\"), log_file=log_file)\n",
    "    logging.basicConfig(\n",
    "                level=logging.INFO,\n",
    "                format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "                handlers=[\n",
    "                    logging.StreamHandler(),           # 同时输出到控制台\n",
    "                    logging.FileHandler(\"application.log\", encoding=\"utf-8\")\n",
    "                ]\n",
    "            )\n",
    "    # 获取 Logger 对象\n",
    "    logger = logging.getLogger(\"Main\")\n",
    "    logger.info(\"日志记录初始化完成。\")\n",
    "\n",
    "\n",
    "    # --------------------- 加载数据 --------------------- #\n",
    "    input_dir = config.get(\"input_dir\", \"./data\")\n",
    "    output_dir = config.get(\"output_dir\", \"./output\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        collected_data, nc = load_clean_save_data(input_dir, output_dir)\n",
    "        logger.info(\"数据加载成功。\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"数据加载失败：{e}\")\n",
    "        return\n",
    "\n",
    "    t = collected_data['t']\n",
    "\n",
    "    # --------------------- 初始化模块 --------------------- #\n",
    "    fitting_module = FittingModule(config=config, logger=logger)\n",
    "    peak_detection_module = PeakDetectionModule(config=config, logger=logger)\n",
    "    main_module = MainModule(config=config)\n",
    "\n",
    "    logger.info(\"模块初始化完成。\")\n",
    "\n",
    "    # --------------------- 执行核心流程 --------------------- #\n",
    "    peaks_results = []\n",
    "    plateaus_results = []\n",
    "\n",
    "    for col in collected_data.columns[1:]:  # 跳过时间列\n",
    "        logger.info(f\"开始处理列：{col}\")\n",
    "        try:\n",
    "            # 数据清洗\n",
    "            t_clean, N_clean = clean_data(t, collected_data[col])\n",
    "\n",
    "            # 动态拟合\n",
    "            fit_result = fitting_module.fit_dynamic(t_clean, N_clean, index=col)\n",
    "            if fit_result:\n",
    "                logger.info(f\"拟合成功：R²={fit_result['r_squared']:.4f}\")\n",
    "            else:\n",
    "                logger.warning(f\"{col} 拟合失败，跳过增长检测。\")\n",
    "                continue\n",
    "            # 峰值检测\n",
    "            peaks_info = peak_detection_module.detect_peaks(N_clean, col)\n",
    "            if peaks_info:\n",
    "                logger.info(f\"{col} 检测到 {len(peaks_info)} 个峰值。\")\n",
    "            else:\n",
    "                logger.warning(f\"{col} 未检测到峰值。\")\n",
    "\n",
    "            # 分析和可视化\n",
    "            plateaus_info = peak_detection_module.plot_analysis_figure(\n",
    "                t_clean, N_clean, peaks_info, col, fit_result\n",
    "            )\n",
    "            if plateaus_info:\n",
    "                logger.info(f\"{col} 检测到 {len(plateaus_info)} 个平台。\")\n",
    "            else:\n",
    "                logger.warning(f\"{col} 未检测到平台。\")\n",
    "            \n",
    "            peaks_results.append(peaks_info)\n",
    "            plateaus_results.append(plateaus_info)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"处理列 {col} 时发生错误：{e}\")\n",
    "\n",
    "    # --------------------- 保存结果 --------------------- #\n",
    "    try:\n",
    "        main_module.save_results_to_csv(peaks_results, plateaus_results)\n",
    "        main_module.plot_overall_analysis(peaks_results, plateaus_results)\n",
    "        logger.info(\"结果保存成功。\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"保存结果失败：{e}\")\n",
    "\n",
    "    logger.info(\"分析流程完成。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 调用主函数\u001b[39;00m\n\u001b[0;32m      2\u001b[0m config_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhusky\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpeek-find\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPEM4Fe\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcool1000\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 配置文件路径\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmain\u001b[49m(config_file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "# 调用主函数\n",
    "config_file = r\"C:\\Users\\husky\\Desktop\\peek-find\\PEM4Fe\\data\\cool1000\\config.yaml\"  # 配置文件路径\n",
    "main(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- 结果输出模块 -------------------------- #\n",
    "def save_results_to_csv(results, output_file):\n",
    "    \"\"\"保存分析结果到 CSV 文件\"\"\"\n",
    "    pass\n",
    "\n",
    "def save_summary_to_log(results, log_file):\n",
    "    \"\"\"保存总结到日志文件\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------- 主逻辑模块 -------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- 主逻辑模块 -------------------------- #\n",
    "def process_column(column, t_clean, N_clean, Nsc, N0, output_dir, fitting_module, peak_module, viz_module):\n",
    "    \"\"\"\n",
    "    处理单列数据：\n",
    "    - 数据清洗\n",
    "    - 拟合分析\n",
    "    - 峰值检测\n",
    "    - 结果可视化\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def main(config_file):\n",
    "    \"\"\"\n",
    "    主程序入口：\n",
    "    - 加载配置与参数\n",
    "    - 数据处理与分析\n",
    "    - 结果保存与输出\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------- 脚本入口 -------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- 脚本入口 -------------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载日志\n",
    "    setup_logging()\n",
    "    \n",
    "    # 加载配置和参数\n",
    "    config_file = \"config.json\"  # 默认配置文件路径\n",
    "    main(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\husky\\Desktop\\peek-find\\PEM4Fe\\src\\PEM4Fe\\__init__.py\", \"rb\") as f:\n",
    "    content = f.read()\n",
    "    if b'\\x00' in content:\n",
    "        print(\"Found null byte!\")\n",
    "    else:\n",
    "        print(\"No null byte found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开文件并移除空字节\n",
    "file_path = r\"C:\\Users\\husky\\Desktop\\peek-find\\PEM4Fe\\src\\PEM4Fe\\__init__.py\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# 移除空字节\n",
    "clean_content = content.replace(b'\\x00', b'')\n",
    "\n",
    "# 将清理后的内容写回文件\n",
    "with open(file_path, \"wb\") as f:\n",
    "    f.write(clean_content)\n",
    "\n",
    "print(\"Null byte removed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
